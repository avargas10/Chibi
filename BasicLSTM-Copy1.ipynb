{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e270556c-ccc7-491d-aa9f-82c35d4db720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af54e0f2-cd5d-4333-aa32-a0743d9d135a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read pickle file\n",
    "pickle_in = open(\"data/plots_text.pickle\",\"rb\")\n",
    "movie_plots = pickle.load(pickle_in)\n",
    "\n",
    "# count of movie plot summaries\n",
    "len(movie_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e456ee0-ce69-43f4-8dce-a0bff3572977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text\n",
    "movie_plots = [re.sub(\"[^a-z' ]\", \"\", i) for i in movie_plots]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "099b58ee-3029-4577-b906-273a082e35aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequences of length 5 tokens\n",
    "def create_seq(text, seq_len = 5):\n",
    "    \n",
    "    sequences = []\n",
    "\n",
    "    # if the number of tokens in 'text' is greater than 5\n",
    "    if len(text.split()) > seq_len:\n",
    "      for i in range(seq_len, len(text.split())):\n",
    "        # select sequence of tokens\n",
    "        seq = text.split()[i-seq_len:i+1]\n",
    "        # add to the list\n",
    "        sequences.append(\" \".join(seq))\n",
    "\n",
    "      return sequences\n",
    "\n",
    "    # if the number of tokens in 'text' is less than or equal to 5\n",
    "    else:\n",
    "      \n",
    "      return [text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "306b7da2-d7eb-4f17-8e73-5280bd5e48b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152644"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs = [create_seq(i) for i in movie_plots]\n",
    "\n",
    "# merge list-of-lists into a single list\n",
    "seqs = sum(seqs, [])\n",
    "\n",
    "# count of sequences\n",
    "len(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e9d8b81-b473-4e11-927c-55d3e5a03e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create inputs and targets (x and y)\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for s in seqs:\n",
    "  x.append(\" \".join(s.split()[:-1]))\n",
    "  y.append(\" \".join(s.split()[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f655efec-466c-4a27-8630-4bafa6f7259e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2427, 'clover')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create integer-to-token mapping\n",
    "int2token = {}\n",
    "cnt = 0\n",
    "\n",
    "for w in set(\" \".join(movie_plots).split()):\n",
    "  int2token[cnt] = w\n",
    "  cnt+= 1\n",
    "\n",
    "# create token-to-integer mapping\n",
    "token2int = {t: i for i, t in int2token.items()}\n",
    "\n",
    "token2int[\"the\"], int2token[12641]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bcbe68e-edf2-48e0-832f-b02b561b1ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16592"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set vocabulary size\n",
    "vocab_size = len(int2token)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b7b797e-b414-4693-a038-4ca4902bd145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_integer_seq(seq):\n",
    "  return [token2int[w] for w in seq.split()]\n",
    "\n",
    "# convert text sequences to integer sequences\n",
    "x_int = [get_integer_seq(i) for i in x]\n",
    "y_int = [get_integer_seq(i) for i in y]\n",
    "\n",
    "# convert lists to numpy arrays\n",
    "x_int = np.array(x_int)\n",
    "y_int = np.array(y_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e48926d7-c6e9-4369-bdf5-f77e15ae9cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "  '''A handy class from the PyTorch ImageNet tutorial''' \n",
    "  def __init__(self):\n",
    "    self.reset()\n",
    "  def reset(self):\n",
    "    self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
    "  def update(self, val, n=1):\n",
    "    self.val = val\n",
    "    self.sum += val * n\n",
    "    self.count += n\n",
    "    self.avg = self.sum / self.count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b669f992-529e-4858-a296-afc755181e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr_x, arr_y, batch_size):\n",
    "         \n",
    "    # iterate through the arrays\n",
    "    prv = 0\n",
    "    for n in range(batch_size, arr_x.shape[0], batch_size):\n",
    "      x = arr_x[prv:n,:]\n",
    "      y = arr_y[prv:n,:]\n",
    "      prv = n\n",
    "      yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3246a047-8bc2-4df3-8b52-1a8b737e01b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=256, n_layers=4, drop_prob=0.3, lr=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.emb_layer = nn.Embedding(vocab_size, 200)\n",
    "\n",
    "        ## define the LSTM\n",
    "        self.lstm = nn.LSTM(200, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## define the fully-connected layer\n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "\n",
    "        ## pass input through embedding layer\n",
    "        embedded = self.emb_layer(x)     \n",
    "        \n",
    "        ## Get the outputs and the new hidden state from the lstm\n",
    "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        ## pass through a dropout layer\n",
    "        out = self.dropout(lstm_output)\n",
    "        \n",
    "        #out = out.contiguous().view(-1, self.n_hidden) \n",
    "        out = out.reshape(-1, self.n_hidden) \n",
    "\n",
    "        ## put \"out\" through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        # if GPU is available\n",
    "        if (torch.cuda.is_available()):\n",
    "          hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        \n",
    "        # if GPU is not available\n",
    "        else:\n",
    "          hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62ad20a6-d113-4eac-a769-ebb9326b3197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordLSTM(\n",
      "  (emb_layer): Embedding(16592, 200)\n",
      "  (lstm): LSTM(200, 256, num_layers=4, batch_first=True, dropout=0.3)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=16592, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# instantiate the model\n",
    "net = WordLSTM()\n",
    "\n",
    "# push the model to GPU (avoid it if you are not using the GPU)\n",
    "net.cuda()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa4d3abb-2811-42e8-9137-68d48e461fec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(net, epochs=10, batch_size=32, lr=0.001, clip=1, print_every=32):\n",
    "    \n",
    "    # optimizer\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    # loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # push model to GPU\n",
    "    net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    for e in range(epochs):\n",
    "\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(x_int, y_int, batch_size):\n",
    "            counter+= 1\n",
    "            \n",
    "            # convert numpy arrays to PyTorch arrays\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            # push tensors to GPU\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "            # detach hidden states\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            final_targets = targets.view(-1)\n",
    "            loss = criterion(output, final_targets.long())\n",
    "\n",
    "            # back-propagate error\n",
    "            loss.backward()\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "\n",
    "            # update weigths\n",
    "            opt.step()            \n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "              print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                    \"Step: {}...\".format(counter),\n",
    "                   \"loss: {}...\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ebf8202-ed0e-4c96-8b56-66e896b745c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 256... loss: 7.328736782073975...\n",
      "Epoch: 1/20... Step: 512... loss: 6.777231693267822...\n",
      "Epoch: 1/20... Step: 768... loss: 7.049412727355957...\n",
      "Epoch: 1/20... Step: 1024... loss: 6.955998420715332...\n",
      "Epoch: 1/20... Step: 1280... loss: 6.927529811859131...\n",
      "Epoch: 1/20... Step: 1536... loss: 7.898260593414307...\n",
      "Epoch: 1/20... Step: 1792... loss: 7.417134761810303...\n",
      "Epoch: 1/20... Step: 2048... loss: 7.538390159606934...\n",
      "Epoch: 1/20... Step: 2304... loss: 6.315855979919434...\n",
      "Epoch: 1/20... Step: 2560... loss: 7.324606418609619...\n",
      "Epoch: 1/20... Step: 2816... loss: 6.902436256408691...\n",
      "Epoch: 1/20... Step: 3072... loss: 7.9458770751953125...\n",
      "Epoch: 1/20... Step: 3328... loss: 7.2466278076171875...\n",
      "Epoch: 1/20... Step: 3584... loss: 6.856001377105713...\n",
      "Epoch: 1/20... Step: 3840... loss: 6.854608058929443...\n",
      "Epoch: 1/20... Step: 4096... loss: 7.889066219329834...\n",
      "Epoch: 1/20... Step: 4352... loss: 7.352333068847656...\n",
      "Epoch: 1/20... Step: 4608... loss: 8.551872253417969...\n",
      "Epoch: 2/20... Step: 4864... loss: 6.298294544219971...\n",
      "Epoch: 2/20... Step: 5120... loss: 5.822021007537842...\n",
      "Epoch: 2/20... Step: 5376... loss: 6.258227348327637...\n",
      "Epoch: 2/20... Step: 5632... loss: 6.223901271820068...\n",
      "Epoch: 2/20... Step: 5888... loss: 6.538933753967285...\n",
      "Epoch: 2/20... Step: 6144... loss: 5.800089359283447...\n",
      "Epoch: 2/20... Step: 6400... loss: 7.33590030670166...\n",
      "Epoch: 2/20... Step: 6656... loss: 6.0338521003723145...\n",
      "Epoch: 2/20... Step: 6912... loss: 7.197091102600098...\n",
      "Epoch: 2/20... Step: 7168... loss: 6.731526851654053...\n",
      "Epoch: 2/20... Step: 7424... loss: 6.770682334899902...\n",
      "Epoch: 2/20... Step: 7680... loss: 7.188490390777588...\n",
      "Epoch: 2/20... Step: 7936... loss: 6.782034873962402...\n",
      "Epoch: 2/20... Step: 8192... loss: 6.155840873718262...\n",
      "Epoch: 2/20... Step: 8448... loss: 6.577637672424316...\n",
      "Epoch: 2/20... Step: 8704... loss: 6.932801723480225...\n",
      "Epoch: 2/20... Step: 8960... loss: 6.678731441497803...\n",
      "Epoch: 2/20... Step: 9216... loss: 6.804374694824219...\n",
      "Epoch: 2/20... Step: 9472... loss: 7.2730584144592285...\n",
      "Epoch: 3/20... Step: 9728... loss: 6.200812339782715...\n",
      "Epoch: 3/20... Step: 9984... loss: 5.109489917755127...\n",
      "Epoch: 3/20... Step: 10240... loss: 5.959147930145264...\n",
      "Epoch: 3/20... Step: 10496... loss: 6.512368202209473...\n",
      "Epoch: 3/20... Step: 10752... loss: 5.444571018218994...\n",
      "Epoch: 3/20... Step: 11008... loss: 6.090546607971191...\n",
      "Epoch: 3/20... Step: 11264... loss: 5.675531387329102...\n",
      "Epoch: 3/20... Step: 11520... loss: 5.344381332397461...\n",
      "Epoch: 3/20... Step: 11776... loss: 5.543026447296143...\n",
      "Epoch: 3/20... Step: 12032... loss: 5.764643669128418...\n",
      "Epoch: 3/20... Step: 12288... loss: 5.995022773742676...\n",
      "Epoch: 3/20... Step: 12544... loss: 6.160470962524414...\n",
      "Epoch: 3/20... Step: 12800... loss: 6.311509132385254...\n",
      "Epoch: 3/20... Step: 13056... loss: 5.692401885986328...\n",
      "Epoch: 3/20... Step: 13312... loss: 6.942244052886963...\n",
      "Epoch: 3/20... Step: 13568... loss: 6.179751873016357...\n",
      "Epoch: 3/20... Step: 13824... loss: 5.464946746826172...\n",
      "Epoch: 3/20... Step: 14080... loss: 6.408663749694824...\n",
      "Epoch: 4/20... Step: 14336... loss: 6.298964500427246...\n",
      "Epoch: 4/20... Step: 14592... loss: 5.800061225891113...\n",
      "Epoch: 4/20... Step: 14848... loss: 5.599527835845947...\n",
      "Epoch: 4/20... Step: 15104... loss: 5.919402122497559...\n",
      "Epoch: 4/20... Step: 15360... loss: 5.419161796569824...\n",
      "Epoch: 4/20... Step: 15616... loss: 5.716601371765137...\n",
      "Epoch: 4/20... Step: 15872... loss: 5.672130584716797...\n",
      "Epoch: 4/20... Step: 16128... loss: 5.457925319671631...\n",
      "Epoch: 4/20... Step: 16384... loss: 4.923178672790527...\n",
      "Epoch: 4/20... Step: 16640... loss: 5.315768241882324...\n",
      "Epoch: 4/20... Step: 16896... loss: 5.640347003936768...\n",
      "Epoch: 4/20... Step: 17152... loss: 6.149484634399414...\n",
      "Epoch: 4/20... Step: 17408... loss: 5.576620101928711...\n",
      "Epoch: 4/20... Step: 17664... loss: 6.4544243812561035...\n",
      "Epoch: 4/20... Step: 17920... loss: 5.863876819610596...\n",
      "Epoch: 4/20... Step: 18176... loss: 6.298654556274414...\n",
      "Epoch: 4/20... Step: 18432... loss: 5.725903511047363...\n",
      "Epoch: 4/20... Step: 18688... loss: 5.722664833068848...\n",
      "Epoch: 4/20... Step: 18944... loss: 5.933554649353027...\n",
      "Epoch: 5/20... Step: 19200... loss: 4.291327476501465...\n",
      "Epoch: 5/20... Step: 19456... loss: 5.1080756187438965...\n",
      "Epoch: 5/20... Step: 19712... loss: 5.498780250549316...\n",
      "Epoch: 5/20... Step: 19968... loss: 5.1447014808654785...\n",
      "Epoch: 5/20... Step: 20224... loss: 5.523185729980469...\n",
      "Epoch: 5/20... Step: 20480... loss: 5.363811016082764...\n",
      "Epoch: 5/20... Step: 20736... loss: 4.208794593811035...\n",
      "Epoch: 5/20... Step: 20992... loss: 4.9216508865356445...\n",
      "Epoch: 5/20... Step: 21248... loss: 4.915823936462402...\n",
      "Epoch: 5/20... Step: 21504... loss: 5.6436238288879395...\n",
      "Epoch: 5/20... Step: 21760... loss: 5.782853126525879...\n",
      "Epoch: 5/20... Step: 22016... loss: 5.144822120666504...\n",
      "Epoch: 5/20... Step: 22272... loss: 5.476954460144043...\n",
      "Epoch: 5/20... Step: 22528... loss: 5.108184814453125...\n",
      "Epoch: 5/20... Step: 22784... loss: 5.392650604248047...\n",
      "Epoch: 5/20... Step: 23040... loss: 5.215886116027832...\n",
      "Epoch: 5/20... Step: 23296... loss: 5.353799343109131...\n",
      "Epoch: 5/20... Step: 23552... loss: 5.136468410491943...\n",
      "Epoch: 5/20... Step: 23808... loss: 5.714189052581787...\n",
      "Epoch: 6/20... Step: 24064... loss: 4.498462200164795...\n",
      "Epoch: 6/20... Step: 24320... loss: 5.398375988006592...\n",
      "Epoch: 6/20... Step: 24576... loss: 5.104984760284424...\n",
      "Epoch: 6/20... Step: 24832... loss: 4.720282077789307...\n",
      "Epoch: 6/20... Step: 25088... loss: 4.77517032623291...\n",
      "Epoch: 6/20... Step: 25344... loss: 5.097411632537842...\n",
      "Epoch: 6/20... Step: 25600... loss: 4.581084251403809...\n",
      "Epoch: 6/20... Step: 25856... loss: 4.600913047790527...\n",
      "Epoch: 6/20... Step: 26112... loss: 5.531386852264404...\n",
      "Epoch: 6/20... Step: 26368... loss: 5.514575004577637...\n",
      "Epoch: 6/20... Step: 26624... loss: 5.364178657531738...\n",
      "Epoch: 6/20... Step: 26880... loss: 5.34782600402832...\n",
      "Epoch: 6/20... Step: 27136... loss: 5.143235206604004...\n",
      "Epoch: 6/20... Step: 27392... loss: 4.512700080871582...\n",
      "Epoch: 6/20... Step: 27648... loss: 4.556105613708496...\n",
      "Epoch: 6/20... Step: 27904... loss: 5.589025974273682...\n",
      "Epoch: 6/20... Step: 28160... loss: 4.855837821960449...\n",
      "Epoch: 6/20... Step: 28416... loss: 5.181040287017822...\n",
      "Epoch: 7/20... Step: 28672... loss: 4.990786552429199...\n",
      "Epoch: 7/20... Step: 28928... loss: 5.040661811828613...\n",
      "Epoch: 7/20... Step: 29184... loss: 5.646819114685059...\n",
      "Epoch: 7/20... Step: 29440... loss: 4.979689121246338...\n",
      "Epoch: 7/20... Step: 29696... loss: 5.083452224731445...\n",
      "Epoch: 7/20... Step: 29952... loss: 4.983546257019043...\n",
      "Epoch: 7/20... Step: 30208... loss: 4.996479034423828...\n",
      "Epoch: 7/20... Step: 30464... loss: 4.486031532287598...\n",
      "Epoch: 7/20... Step: 30720... loss: 5.064204692840576...\n",
      "Epoch: 7/20... Step: 30976... loss: 5.440090656280518...\n",
      "Epoch: 7/20... Step: 31232... loss: 5.444027423858643...\n",
      "Epoch: 7/20... Step: 31488... loss: 4.889034748077393...\n",
      "Epoch: 7/20... Step: 31744... loss: 5.133085250854492...\n",
      "Epoch: 7/20... Step: 32000... loss: 5.6263933181762695...\n",
      "Epoch: 7/20... Step: 32256... loss: 4.535939693450928...\n",
      "Epoch: 7/20... Step: 32512... loss: 4.891582489013672...\n",
      "Epoch: 7/20... Step: 32768... loss: 4.231116771697998...\n",
      "Epoch: 7/20... Step: 33024... loss: 5.815718173980713...\n",
      "Epoch: 7/20... Step: 33280... loss: 5.360898017883301...\n",
      "Epoch: 8/20... Step: 33536... loss: 4.314308166503906...\n",
      "Epoch: 8/20... Step: 33792... loss: 5.034581184387207...\n",
      "Epoch: 8/20... Step: 34048... loss: 4.715704917907715...\n",
      "Epoch: 8/20... Step: 34304... loss: 5.119823455810547...\n",
      "Epoch: 8/20... Step: 34560... loss: 4.279755592346191...\n",
      "Epoch: 8/20... Step: 34816... loss: 5.3087663650512695...\n",
      "Epoch: 8/20... Step: 35072... loss: 4.062763690948486...\n",
      "Epoch: 8/20... Step: 35328... loss: 4.359199047088623...\n",
      "Epoch: 8/20... Step: 35584... loss: 5.928193092346191...\n",
      "Epoch: 8/20... Step: 35840... loss: 4.29059362411499...\n",
      "Epoch: 8/20... Step: 36096... loss: 5.083352088928223...\n",
      "Epoch: 8/20... Step: 36352... loss: 5.160513877868652...\n",
      "Epoch: 8/20... Step: 36608... loss: 4.572340488433838...\n",
      "Epoch: 8/20... Step: 36864... loss: 4.816152095794678...\n",
      "Epoch: 8/20... Step: 37120... loss: 5.470532417297363...\n",
      "Epoch: 8/20... Step: 37376... loss: 4.850983619689941...\n",
      "Epoch: 8/20... Step: 37632... loss: 5.082192897796631...\n",
      "Epoch: 8/20... Step: 37888... loss: 5.587484836578369...\n",
      "Epoch: 8/20... Step: 38144... loss: 4.965357780456543...\n",
      "Epoch: 9/20... Step: 38400... loss: 4.66054630279541...\n",
      "Epoch: 9/20... Step: 38656... loss: 4.965878963470459...\n",
      "Epoch: 9/20... Step: 38912... loss: 4.552064895629883...\n",
      "Epoch: 9/20... Step: 39168... loss: 4.615665912628174...\n",
      "Epoch: 9/20... Step: 39424... loss: 4.610821723937988...\n",
      "Epoch: 9/20... Step: 39680... loss: 4.673673152923584...\n",
      "Epoch: 9/20... Step: 39936... loss: 4.661246299743652...\n",
      "Epoch: 9/20... Step: 40192... loss: 4.882361888885498...\n",
      "Epoch: 9/20... Step: 40448... loss: 5.000659942626953...\n",
      "Epoch: 9/20... Step: 40704... loss: 4.742902755737305...\n",
      "Epoch: 9/20... Step: 40960... loss: 4.879985809326172...\n",
      "Epoch: 9/20... Step: 41216... loss: 5.678334712982178...\n",
      "Epoch: 9/20... Step: 41472... loss: 4.5854411125183105...\n",
      "Epoch: 9/20... Step: 41728... loss: 5.228918075561523...\n",
      "Epoch: 9/20... Step: 41984... loss: 4.816627502441406...\n",
      "Epoch: 9/20... Step: 42240... loss: 4.88350772857666...\n",
      "Epoch: 9/20... Step: 42496... loss: 4.9944939613342285...\n",
      "Epoch: 9/20... Step: 42752... loss: 5.776158809661865...\n",
      "Epoch: 10/20... Step: 43008... loss: 4.3837056159973145...\n",
      "Epoch: 10/20... Step: 43264... loss: 5.262031555175781...\n",
      "Epoch: 10/20... Step: 43520... loss: 4.620489597320557...\n",
      "Epoch: 10/20... Step: 43776... loss: 4.130815505981445...\n",
      "Epoch: 10/20... Step: 44032... loss: 5.0680389404296875...\n",
      "Epoch: 10/20... Step: 44288... loss: 4.363670349121094...\n",
      "Epoch: 10/20... Step: 44544... loss: 5.342790126800537...\n",
      "Epoch: 10/20... Step: 44800... loss: 4.727989196777344...\n",
      "Epoch: 10/20... Step: 45056... loss: 5.147243499755859...\n",
      "Epoch: 10/20... Step: 45312... loss: 5.5392584800720215...\n",
      "Epoch: 10/20... Step: 45568... loss: 4.257057189941406...\n",
      "Epoch: 10/20... Step: 45824... loss: 5.494658946990967...\n",
      "Epoch: 10/20... Step: 46080... loss: 4.555538654327393...\n",
      "Epoch: 10/20... Step: 46336... loss: 4.548133850097656...\n",
      "Epoch: 10/20... Step: 46592... loss: 5.662520408630371...\n",
      "Epoch: 10/20... Step: 46848... loss: 4.663217067718506...\n",
      "Epoch: 10/20... Step: 47104... loss: 4.841307163238525...\n",
      "Epoch: 10/20... Step: 47360... loss: 5.55637264251709...\n",
      "Epoch: 10/20... Step: 47616... loss: 4.60659646987915...\n",
      "Epoch: 11/20... Step: 47872... loss: 4.084313869476318...\n",
      "Epoch: 11/20... Step: 48128... loss: 4.94744873046875...\n",
      "Epoch: 11/20... Step: 48384... loss: 4.97659969329834...\n",
      "Epoch: 11/20... Step: 48640... loss: 4.35721492767334...\n",
      "Epoch: 11/20... Step: 48896... loss: 5.085092544555664...\n",
      "Epoch: 11/20... Step: 49152... loss: 4.175387859344482...\n",
      "Epoch: 11/20... Step: 49408... loss: 4.395503044128418...\n",
      "Epoch: 11/20... Step: 49664... loss: 4.907593727111816...\n",
      "Epoch: 11/20... Step: 49920... loss: 5.544705390930176...\n",
      "Epoch: 11/20... Step: 50176... loss: 4.723902702331543...\n",
      "Epoch: 11/20... Step: 50432... loss: 4.62534236907959...\n",
      "Epoch: 11/20... Step: 50688... loss: 4.4962005615234375...\n",
      "Epoch: 11/20... Step: 50944... loss: 4.919776916503906...\n",
      "Epoch: 11/20... Step: 51200... loss: 4.634310722351074...\n",
      "Epoch: 11/20... Step: 51456... loss: 4.779486656188965...\n",
      "Epoch: 11/20... Step: 51712... loss: 4.559934139251709...\n",
      "Epoch: 11/20... Step: 51968... loss: 4.928205966949463...\n",
      "Epoch: 11/20... Step: 52224... loss: 4.797836780548096...\n",
      "Epoch: 12/20... Step: 52480... loss: 4.7595295906066895...\n",
      "Epoch: 12/20... Step: 52736... loss: 4.391053199768066...\n",
      "Epoch: 12/20... Step: 52992... loss: 4.904852390289307...\n",
      "Epoch: 12/20... Step: 53248... loss: 4.8677520751953125...\n",
      "Epoch: 12/20... Step: 53504... loss: 4.188166618347168...\n",
      "Epoch: 12/20... Step: 53760... loss: 4.9175801277160645...\n",
      "Epoch: 12/20... Step: 54016... loss: 4.814245700836182...\n",
      "Epoch: 12/20... Step: 54272... loss: 5.116002559661865...\n",
      "Epoch: 12/20... Step: 54528... loss: 4.650824069976807...\n",
      "Epoch: 12/20... Step: 54784... loss: 4.183273792266846...\n",
      "Epoch: 12/20... Step: 55040... loss: 4.127337455749512...\n",
      "Epoch: 12/20... Step: 55296... loss: 4.373067378997803...\n",
      "Epoch: 12/20... Step: 55552... loss: 4.3507609367370605...\n",
      "Epoch: 12/20... Step: 55808... loss: 4.757205009460449...\n",
      "Epoch: 12/20... Step: 56064... loss: 5.187228679656982...\n",
      "Epoch: 12/20... Step: 56320... loss: 3.941218137741089...\n",
      "Epoch: 12/20... Step: 56576... loss: 4.813760280609131...\n",
      "Epoch: 12/20... Step: 56832... loss: 4.601581573486328...\n",
      "Epoch: 12/20... Step: 57088... loss: 4.8565778732299805...\n",
      "Epoch: 13/20... Step: 57344... loss: 4.522329330444336...\n",
      "Epoch: 13/20... Step: 57600... loss: 4.397564888000488...\n",
      "Epoch: 13/20... Step: 57856... loss: 4.227108955383301...\n",
      "Epoch: 13/20... Step: 58112... loss: 3.6965408325195312...\n",
      "Epoch: 13/20... Step: 58368... loss: 4.8963470458984375...\n",
      "Epoch: 13/20... Step: 58624... loss: 4.925604343414307...\n",
      "Epoch: 13/20... Step: 58880... loss: 4.596161842346191...\n",
      "Epoch: 13/20... Step: 59136... loss: 5.389707088470459...\n",
      "Epoch: 13/20... Step: 59392... loss: 4.097146034240723...\n",
      "Epoch: 13/20... Step: 59648... loss: 4.9526753425598145...\n",
      "Epoch: 13/20... Step: 59904... loss: 4.357316970825195...\n",
      "Epoch: 13/20... Step: 60160... loss: 4.809899806976318...\n",
      "Epoch: 13/20... Step: 60416... loss: 4.476265907287598...\n",
      "Epoch: 13/20... Step: 60672... loss: 5.278912544250488...\n",
      "Epoch: 13/20... Step: 60928... loss: 4.183411121368408...\n",
      "Epoch: 13/20... Step: 61184... loss: 4.275816917419434...\n",
      "Epoch: 13/20... Step: 61440... loss: 4.169949531555176...\n",
      "Epoch: 13/20... Step: 61696... loss: 4.16127347946167...\n",
      "Epoch: 13/20... Step: 61952... loss: 4.4410600662231445...\n",
      "Epoch: 14/20... Step: 62208... loss: 4.166615009307861...\n",
      "Epoch: 14/20... Step: 62464... loss: 4.173659324645996...\n",
      "Epoch: 14/20... Step: 62720... loss: 4.800955772399902...\n",
      "Epoch: 14/20... Step: 62976... loss: 4.764115810394287...\n",
      "Epoch: 14/20... Step: 63232... loss: 4.695387363433838...\n",
      "Epoch: 14/20... Step: 63488... loss: 4.563666343688965...\n",
      "Epoch: 14/20... Step: 63744... loss: 4.921189785003662...\n",
      "Epoch: 14/20... Step: 64000... loss: 4.822839260101318...\n",
      "Epoch: 14/20... Step: 64256... loss: 5.148465633392334...\n",
      "Epoch: 14/20... Step: 64512... loss: 4.820693492889404...\n",
      "Epoch: 14/20... Step: 64768... loss: 3.655411958694458...\n",
      "Epoch: 14/20... Step: 65024... loss: 4.350220203399658...\n",
      "Epoch: 14/20... Step: 65280... loss: 4.738385200500488...\n",
      "Epoch: 14/20... Step: 65536... loss: 4.308648109436035...\n",
      "Epoch: 14/20... Step: 65792... loss: 4.056410789489746...\n",
      "Epoch: 14/20... Step: 66048... loss: 4.415452480316162...\n",
      "Epoch: 14/20... Step: 66304... loss: 5.178624629974365...\n",
      "Epoch: 14/20... Step: 66560... loss: 4.893307209014893...\n",
      "Epoch: 15/20... Step: 66816... loss: 4.010867118835449...\n",
      "Epoch: 15/20... Step: 67072... loss: 5.11801815032959...\n",
      "Epoch: 15/20... Step: 67328... loss: 3.9307732582092285...\n",
      "Epoch: 15/20... Step: 67584... loss: 3.7391304969787598...\n",
      "Epoch: 15/20... Step: 67840... loss: 4.384143352508545...\n",
      "Epoch: 15/20... Step: 68096... loss: 4.404947280883789...\n",
      "Epoch: 15/20... Step: 68352... loss: 4.856019020080566...\n",
      "Epoch: 15/20... Step: 68608... loss: 4.311511039733887...\n",
      "Epoch: 15/20... Step: 68864... loss: 4.576467990875244...\n",
      "Epoch: 15/20... Step: 69120... loss: 4.418110370635986...\n",
      "Epoch: 15/20... Step: 69376... loss: 4.542905807495117...\n",
      "Epoch: 15/20... Step: 69632... loss: 4.780750751495361...\n",
      "Epoch: 15/20... Step: 69888... loss: 4.509561061859131...\n",
      "Epoch: 15/20... Step: 70144... loss: 5.007962226867676...\n",
      "Epoch: 15/20... Step: 70400... loss: 3.5261807441711426...\n",
      "Epoch: 15/20... Step: 70656... loss: 3.7200424671173096...\n",
      "Epoch: 15/20... Step: 70912... loss: 4.3755998611450195...\n",
      "Epoch: 15/20... Step: 71168... loss: 4.376905918121338...\n",
      "Epoch: 15/20... Step: 71424... loss: 4.685677528381348...\n",
      "Epoch: 16/20... Step: 71680... loss: 4.767001152038574...\n",
      "Epoch: 16/20... Step: 71936... loss: 4.026154518127441...\n",
      "Epoch: 16/20... Step: 72192... loss: 4.7525410652160645...\n",
      "Epoch: 16/20... Step: 72448... loss: 4.679844856262207...\n",
      "Epoch: 16/20... Step: 72704... loss: 4.76265811920166...\n",
      "Epoch: 16/20... Step: 72960... loss: 4.480881690979004...\n",
      "Epoch: 16/20... Step: 73216... loss: 4.067452430725098...\n",
      "Epoch: 16/20... Step: 73472... loss: 3.7773139476776123...\n",
      "Epoch: 16/20... Step: 73728... loss: 4.207005500793457...\n",
      "Epoch: 16/20... Step: 73984... loss: 4.086314678192139...\n",
      "Epoch: 16/20... Step: 74240... loss: 4.222337245941162...\n",
      "Epoch: 16/20... Step: 74496... loss: 3.914275646209717...\n",
      "Epoch: 16/20... Step: 74752... loss: 4.27987813949585...\n",
      "Epoch: 16/20... Step: 75008... loss: 4.713959693908691...\n",
      "Epoch: 16/20... Step: 75264... loss: 4.538552284240723...\n",
      "Epoch: 16/20... Step: 75520... loss: 5.154278755187988...\n",
      "Epoch: 16/20... Step: 75776... loss: 4.616124153137207...\n",
      "Epoch: 16/20... Step: 76032... loss: 4.628632545471191...\n",
      "Epoch: 16/20... Step: 76288... loss: 4.761040210723877...\n",
      "Epoch: 17/20... Step: 76544... loss: 4.557837009429932...\n",
      "Epoch: 17/20... Step: 76800... loss: 4.24888277053833...\n",
      "Epoch: 17/20... Step: 77056... loss: 4.4039692878723145...\n",
      "Epoch: 17/20... Step: 77312... loss: 4.165358543395996...\n",
      "Epoch: 17/20... Step: 77568... loss: 4.259178161621094...\n",
      "Epoch: 17/20... Step: 77824... loss: 5.06173038482666...\n",
      "Epoch: 17/20... Step: 78080... loss: 4.3085618019104...\n",
      "Epoch: 17/20... Step: 78336... loss: 4.579019069671631...\n",
      "Epoch: 17/20... Step: 78592... loss: 3.720874309539795...\n",
      "Epoch: 17/20... Step: 78848... loss: 3.5297107696533203...\n",
      "Epoch: 17/20... Step: 79104... loss: 3.6932034492492676...\n",
      "Epoch: 17/20... Step: 79360... loss: 4.223355770111084...\n",
      "Epoch: 17/20... Step: 79616... loss: 3.632111072540283...\n",
      "Epoch: 17/20... Step: 79872... loss: 4.019725799560547...\n",
      "Epoch: 17/20... Step: 80128... loss: 4.134255886077881...\n",
      "Epoch: 17/20... Step: 80384... loss: 4.248946666717529...\n",
      "Epoch: 17/20... Step: 80640... loss: 3.853790283203125...\n",
      "Epoch: 17/20... Step: 80896... loss: 4.757877349853516...\n",
      "Epoch: 18/20... Step: 81152... loss: 4.388258457183838...\n",
      "Epoch: 18/20... Step: 81408... loss: 4.436931133270264...\n",
      "Epoch: 18/20... Step: 81664... loss: 4.2794880867004395...\n",
      "Epoch: 18/20... Step: 81920... loss: 4.401151657104492...\n",
      "Epoch: 18/20... Step: 82176... loss: 4.2086076736450195...\n",
      "Epoch: 18/20... Step: 82432... loss: 3.845264434814453...\n",
      "Epoch: 18/20... Step: 82688... loss: 3.865081787109375...\n",
      "Epoch: 18/20... Step: 82944... loss: 4.649545669555664...\n",
      "Epoch: 18/20... Step: 83200... loss: 4.152987957000732...\n",
      "Epoch: 18/20... Step: 83456... loss: 4.140997409820557...\n",
      "Epoch: 18/20... Step: 83712... loss: 3.8614001274108887...\n",
      "Epoch: 18/20... Step: 83968... loss: 4.041046142578125...\n",
      "Epoch: 18/20... Step: 84224... loss: 4.3013458251953125...\n",
      "Epoch: 18/20... Step: 84480... loss: 3.644257068634033...\n",
      "Epoch: 18/20... Step: 84736... loss: 4.291952133178711...\n",
      "Epoch: 18/20... Step: 84992... loss: 4.029256820678711...\n",
      "Epoch: 18/20... Step: 85248... loss: 4.586472034454346...\n",
      "Epoch: 18/20... Step: 85504... loss: 4.813787937164307...\n",
      "Epoch: 18/20... Step: 85760... loss: 4.775273323059082...\n",
      "Epoch: 19/20... Step: 86016... loss: 3.323809862136841...\n",
      "Epoch: 19/20... Step: 86272... loss: 4.1615729331970215...\n",
      "Epoch: 19/20... Step: 86528... loss: 4.0656585693359375...\n",
      "Epoch: 19/20... Step: 86784... loss: 4.490123748779297...\n",
      "Epoch: 19/20... Step: 87040... loss: 4.393539905548096...\n",
      "Epoch: 19/20... Step: 87296... loss: 4.115165710449219...\n",
      "Epoch: 19/20... Step: 87552... loss: 3.5202198028564453...\n",
      "Epoch: 19/20... Step: 87808... loss: 4.518259525299072...\n",
      "Epoch: 19/20... Step: 88064... loss: 3.9492897987365723...\n",
      "Epoch: 19/20... Step: 88320... loss: 4.446423530578613...\n",
      "Epoch: 19/20... Step: 88576... loss: 4.027905464172363...\n",
      "Epoch: 19/20... Step: 88832... loss: 3.948383331298828...\n",
      "Epoch: 19/20... Step: 89088... loss: 3.727964401245117...\n",
      "Epoch: 19/20... Step: 89344... loss: 4.712838172912598...\n",
      "Epoch: 19/20... Step: 89600... loss: 4.637092113494873...\n",
      "Epoch: 19/20... Step: 89856... loss: 4.622598648071289...\n",
      "Epoch: 19/20... Step: 90112... loss: 4.0008745193481445...\n",
      "Epoch: 19/20... Step: 90368... loss: 4.673806190490723...\n",
      "Epoch: 19/20... Step: 90624... loss: 4.076946258544922...\n",
      "Epoch: 20/20... Step: 90880... loss: 4.072242736816406...\n",
      "Epoch: 20/20... Step: 91136... loss: 4.37141227722168...\n",
      "Epoch: 20/20... Step: 91392... loss: 4.199773788452148...\n",
      "Epoch: 20/20... Step: 91648... loss: 4.447512149810791...\n",
      "Epoch: 20/20... Step: 91904... loss: 4.11993932723999...\n",
      "Epoch: 20/20... Step: 92160... loss: 3.892918109893799...\n",
      "Epoch: 20/20... Step: 92416... loss: 4.092955589294434...\n",
      "Epoch: 20/20... Step: 92672... loss: 4.547211170196533...\n",
      "Epoch: 20/20... Step: 92928... loss: 4.351545810699463...\n",
      "Epoch: 20/20... Step: 93184... loss: 4.2967658042907715...\n",
      "Epoch: 20/20... Step: 93440... loss: 3.615619659423828...\n",
      "Epoch: 20/20... Step: 93696... loss: 4.879213809967041...\n",
      "Epoch: 20/20... Step: 93952... loss: 3.623013973236084...\n",
      "Epoch: 20/20... Step: 94208... loss: 3.9011034965515137...\n",
      "Epoch: 20/20... Step: 94464... loss: 4.3512773513793945...\n",
      "Epoch: 20/20... Step: 94720... loss: 4.379586219787598...\n",
      "Epoch: 20/20... Step: 94976... loss: 4.2043681144714355...\n",
      "Epoch: 20/20... Step: 95232... loss: 4.243649482727051...\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "train(net, batch_size = 32, epochs=20, print_every=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dde620fa-a97f-4e22-ad38-4dcf8b58868c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict next token\n",
    "def predict(net, tkn, h=None):\n",
    "         \n",
    "  # tensor inputs\n",
    "  x = np.array([[token2int[tkn]]])\n",
    "  inputs = torch.from_numpy(x)\n",
    "  \n",
    "  # push to GPU\n",
    "  inputs = inputs.cuda()\n",
    "\n",
    "  # detach hidden state from history\n",
    "  h = tuple([each.data for each in h])\n",
    "\n",
    "  # get the output of the model\n",
    "  out, h = net(inputs, h)\n",
    "\n",
    "  # get the token probabilities\n",
    "  p = F.softmax(out, dim=1).data\n",
    "\n",
    "  p = p.cpu()\n",
    "\n",
    "  p = p.numpy()\n",
    "  p = p.reshape(p.shape[1],)\n",
    "\n",
    "  # get indices of top 3 values\n",
    "  top_n_idx = p.argsort()[-3:][::-1]\n",
    "\n",
    "  # randomly select one of the three indices\n",
    "  sampled_token_index = top_n_idx[random.sample([0,1,2],1)[0]]\n",
    "\n",
    "  # return the encoded value of the predicted char and the hidden state\n",
    "  return int2token[sampled_token_index], h\n",
    "\n",
    "\n",
    "# function to generate text\n",
    "def sample(net, size, prime='it is'):\n",
    "        \n",
    "    # push to GPU\n",
    "    net.cuda()\n",
    "    \n",
    "    net.eval()\n",
    "\n",
    "    # batch size is 1\n",
    "    h = net.init_hidden(1)\n",
    "\n",
    "    toks = prime.split()\n",
    "\n",
    "    # predict next token\n",
    "    for t in prime.split():\n",
    "      token, h = predict(net, t, h)\n",
    "    \n",
    "    toks.append(token)\n",
    "\n",
    "    # predict subsequent tokens\n",
    "    for i in range(size-1):\n",
    "        token, h = predict(net, toks[-1], h)\n",
    "        toks.append(token)\n",
    "\n",
    "    return ' '.join(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9902438-8da0-4cf0-a668-f17e9e701f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it is a vampire he has been able to cope in his life the next morning the'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(net, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c1054f9-eaaf-43a0-a624-253dafe193ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one of the deer states that she has to take him down a halfvampire dj x and the'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(net, 15, prime = \"one of the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fddf180-a9ed-4d38-8c41-ec13cf99e023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'as soon as a result she is now the first man is the real alpha railroad hitman the'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(net, 15, prime = \"as soon as\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2e5d27c-36c8-4093-b68b-24587a55a552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'they are interrupted with a ferocious animal in a public accident alan arrives at the train'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(net, 15, prime = \"they\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e597d808-e4d8-4a94-bc2e-5cb2051dbe03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
